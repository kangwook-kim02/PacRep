Using one GPU: cuda.
Namespace(anchor='anchor', auto_clean_model_mode=1, batch_size=6, bert_config_path='./config/uncased_L-12_H-768_A-12/bert_config.json', bert_is_array=True, bert_pretrained_path='./saved_model/english_bert_base.bin', bert_vocab_path='./config/uncased_L-12_H-768_A-12/vocab.txt', breakpoint=-1, config_path='./config/model-en.conf', data_dir='./data/MULTI', dim_bert=768, dim_hidden=512, dp_mode_gpu_num=-1, embedding_dropout_rate=0.5, gpu_mode=1, has_valid=1, iter_num=100000, label_path='./config/label.conf', lambda1=1, lambda2=1, language='en', learning_rate=0.001, linear_dropout_rate=0.5, local_rank=0, lr_bert=1e-05, lr_word_vector=0.0001, max_checkpoint=4096, max_length_sen=256, model_type='BertConCap', multi_gpu=None, n_gram=3, n_loss=3, name_model='MULTI', optim_type='Adam', per_checkpoint=256, pretrained_model_path='./saved_model/model-f1_macro-650.pth', rnn_type='LSTM', sampled_num=10000, seed=2021, segment_type='char', use_pretrain_bert=True, use_word_vec=False, weight_decay=0)
Traceback (most recent call last):
  File "run_train.py", line 104, in <module>
    train_model = TrainModel(config, device=device)
  File "/data/multi_depart/pacrep/deepnet/train.py", line 43, in __init__
    self.model = RecognitionModel(config, device=device) # config에 지정된 모델 객체 생성
  File "/data/multi_depart/pacrep/deepnet/recognition.py", line 83, in __init__
    self.model.to(device)
  File "/home/multi_depart/anaconda3/envs/pacrep/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1173, in to
    return self._apply(convert)
  File "/home/multi_depart/anaconda3/envs/pacrep/lib/python3.8/site-packages/torch/nn/modules/module.py", line 779, in _apply
    module._apply(fn)
  File "/home/multi_depart/anaconda3/envs/pacrep/lib/python3.8/site-packages/torch/nn/modules/module.py", line 779, in _apply
    module._apply(fn)
  File "/home/multi_depart/anaconda3/envs/pacrep/lib/python3.8/site-packages/torch/nn/modules/module.py", line 779, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/home/multi_depart/anaconda3/envs/pacrep/lib/python3.8/site-packages/torch/nn/modules/module.py", line 804, in _apply
    param_applied = fn(param)
  File "/home/multi_depart/anaconda3/envs/pacrep/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1159, in convert
    return t.to(
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Using one GPU: cuda.
Namespace(anchor='anchor', auto_clean_model_mode=1, batch_size=6, bert_config_path='./config/uncased_L-12_H-768_A-12/bert_config.json', bert_is_array=True, bert_pretrained_path='./saved_model/english_bert_base.bin', bert_vocab_path='./config/uncased_L-12_H-768_A-12/vocab.txt', breakpoint=-1, config_path='./config/model-en.conf', data_dir='./data/MULTI', dim_bert=768, dim_hidden=512, dp_mode_gpu_num=-1, embedding_dropout_rate=0.5, gpu_mode=1, has_valid=1, iter_num=100000, label_path='./config/label.conf', lambda1=1, lambda2=1, language='en', learning_rate=0.001, linear_dropout_rate=0.5, local_rank=0, lr_bert=1e-05, lr_word_vector=0.0001, max_checkpoint=4096, max_length_sen=256, model_type='BertConCap', multi_gpu=None, n_gram=3, n_loss=3, name_model='MULTI', optim_type='Adam', per_checkpoint=256, pretrained_model_path='./saved_model/model-f1_macro-650.pth', rnn_type='LSTM', sampled_num=10000, seed=2021, segment_type='char', use_pretrain_bert=True, use_word_vec=False, weight_decay=0)
Traceback (most recent call last):
  File "run_train.py", line 105, in <module>
    train_model.run(config)
  File "/data/multi_depart/pacrep/deepnet/train.py", line 132, in run
    loss_step = loss_step + TrainModel.train(self.model, self.data_train) / config.per_checkpoint
  File "/data/multi_depart/pacrep/deepnet/train.py", line 137, in train
    loss = model.step_train(batched_data)
  File "/data/multi_depart/pacrep/deepnet/recognition.py", line 131, in step_train
    loss_general, loss_cont, dict_probs, _ = self.model(batched_data_torch, self.ignore_idx, is_train=True)
  File "/home/multi_depart/anaconda3/envs/pacrep/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/multi_depart/anaconda3/envs/pacrep/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/multi_depart/pacrep/deepnet/model/models.py", line 101, in forward
    dict_probs, dict_sims, reps = self.predict(batched_data, is_train)
  File "/data/multi_depart/pacrep/deepnet/model/models.py", line 63, in predict
    rep_text, output_tensor_pad = self.bert(list_tokens, list_lens)
  File "/home/multi_depart/anaconda3/envs/pacrep/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/multi_depart/anaconda3/envs/pacrep/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/multi_depart/pacrep/deepnet/word_bert/word_bert.py", line 33, in forward
    rep_text, rep_sub_words = self.bert(tokens, segment_ids, attn_masks)
  File "/home/multi_depart/anaconda3/envs/pacrep/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/multi_depart/anaconda3/envs/pacrep/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/multi_depart/pacrep/deepnet/word_bert/bert_models.py", line 70, in forward
    _, pooled_output, sequence_output = self.bert(input_ids, token_type_ids, attention_mask)
  File "/home/multi_depart/anaconda3/envs/pacrep/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/multi_depart/anaconda3/envs/pacrep/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/multi_depart/pacrep/deepnet/word_bert/bert_models.py", line 31, in forward
    all_encoder_layers = self.encoder(embedding_output, extended_attention_mask)
  File "/home/multi_depart/anaconda3/envs/pacrep/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/multi_depart/anaconda3/envs/pacrep/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/multi_depart/pacrep/deepnet/word_bert/huggingface/modeling.py", line 289, in forward
    hidden_states = layer_module(hidden_states, attention_mask)
  File "/home/multi_depart/anaconda3/envs/pacrep/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/multi_depart/anaconda3/envs/pacrep/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/multi_depart/pacrep/deepnet/word_bert/huggingface/modeling.py", line 274, in forward
    attention_output = self.attention(hidden_states, attention_mask)
  File "/home/multi_depart/anaconda3/envs/pacrep/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/multi_depart/anaconda3/envs/pacrep/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/multi_depart/pacrep/deepnet/word_bert/huggingface/modeling.py", line 234, in forward
    self_output = self.self(input_tensor, attention_mask)
  File "/home/multi_depart/anaconda3/envs/pacrep/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/multi_depart/anaconda3/envs/pacrep/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/multi_depart/pacrep/deepnet/word_bert/huggingface/modeling.py", line 194, in forward
    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 72.00 MiB. GPU 
